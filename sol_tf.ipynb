{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sol_tf.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MDZEM2ayqkSS",
        "DCpbxGEHqrOe",
        "4B7p84stq6PR",
        "g87Ab40FrMQp",
        "UA18v__rrTly",
        "BQz190rRrYwz"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivekverma1019/start-of-line-finder/blob/master/sol_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JYyjJxucWG1",
        "colab_type": "text"
      },
      "source": [
        "#### mount gdrive and import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KPH9P8oJOkW",
        "colab_type": "code",
        "outputId": "c8196a03-bf37-468a-acfd-a2ac835fd06b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMsKEb0HmiSn",
        "colab_type": "code",
        "outputId": "4bd78b29-de84-4300-ab10-7275f8036a10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da7wO--cpaR2",
        "colab_type": "code",
        "outputId": "730c057b-02a9-46e2-e4eb-59b62f305d60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.enable_eager_execution()\n",
        "tf.executing_eagerly()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA2DF4pYgJ-Q",
        "colab_type": "code",
        "outputId": "837b75d9-0968-432d-841f-eb1f97e40d6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 18206171394673884143, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 13052276355200035655\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 15955913154332834814\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15956161332\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 12860394930174473951\n",
              " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDZEM2ayqkSS",
        "colab_type": "text"
      },
      "source": [
        "#### json_state(path)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdGZzcHzKlpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import json\n",
        "def json_state(path):\n",
        "    for i in range(10):\n",
        "        try:\n",
        "            with open(path) as f:\n",
        "                state = json.load(f)\n",
        "            return state\n",
        "        except:\n",
        "            print(\"Failed to load\",i,path)\n",
        "            time.sleep(i)\n",
        "            pass\n",
        "    print(\"Failed to load state\")\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCpbxGEHqrOe",
        "colab_type": "text"
      },
      "source": [
        "#### generate_random_crop(img , gt , params)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_IZEH4QN-XN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "def perform_crop(img, crop):\n",
        "    cs = crop['crop_size']\n",
        "    cropped_gt_img = img[crop['dim0'][0]:crop['dim0'][1], crop['dim1'][0]:crop['dim1'][1]]\n",
        "    scaled_gt_img = cv2.resize(cropped_gt_img, (cs, cs), interpolation = cv2.INTER_CUBIC)\n",
        "    return scaled_gt_img\n",
        "\n",
        "\n",
        "def generate_random_crop(img, gt, params):\n",
        "    contains_label = np.random.random() < params['prob_label']\n",
        "    cs = params['crop_size']\n",
        "\n",
        "    cnt = 0\n",
        "    while True:\n",
        "\n",
        "        dim0 = np.random.randint(0,img.shape[0]-cs)\n",
        "        dim1 = np.random.randint(0,img.shape[1]-cs)\n",
        "        crop = {\n",
        "            \"dim0\": [dim0, dim0+cs],\n",
        "            \"dim1\": [dim1, dim1+cs],\n",
        "            \"crop_size\": cs\n",
        "        }\n",
        "        #TODO: this only works for the center points\n",
        "        gt_match = np.zeros_like(gt[...,0:2])\n",
        "        gt_match[...,0][gt[...,0] < dim1] = 1\n",
        "        gt_match[...,0][gt[...,0] > dim1+cs] = 1\n",
        "        gt_match[...,1][gt[...,1] < dim0] = 1\n",
        "        gt_match[...,1][gt[...,1] > dim0+cs] = 1\n",
        "        gt_match = 1-gt_match\n",
        "        gt_match = np.logical_and(gt_match[...,0], gt_match[...,1])\n",
        "        if gt_match.sum() > 0 and contains_label or cnt > 100:\n",
        "            #print(\"1st if loop exec.\")\n",
        "            cropped_gt_img = perform_crop(img, crop)\n",
        "            return crop, cropped_gt_img, np.where(gt_match != 0)\n",
        "\n",
        "        if gt_match.sum() == 0 and not contains_label:\n",
        "            #print(\"second if loop exec.\")\n",
        "            cropped_gt_img = perform_crop(img, crop)\n",
        "            return crop, cropped_gt_img, np.where(gt_match != 0)\n",
        "        cnt += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q71fmhyQMEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import yaml\n",
        "with open('/content/drive/My Drive/Downloads/start_follow_read/sample_config.yaml') as f:\n",
        "    config = yaml.load(f)\n",
        "pretrain_config = config['pretraining']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B7p84stq6PR",
        "colab_type": "text"
      },
      "source": [
        "#### what could be the max no. of sol"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YvV7QgB4CH3",
        "colab_type": "code",
        "outputId": "cfb050c9-d91e-4263-85ab-b10701328263",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tmp = []\n",
        "for i in range(50):\n",
        "    if i < 9:\n",
        "        with open('/content/drive/My Drive/Downloads/start_follow_read/data/train_a/00000'+str(i+1)+'/00000'+str(i+1)+'.json') as gt:\n",
        "            gt_json = json.load(gt)\n",
        "            x = [True for i in gt_json if 'sol' in i]\n",
        "            tmp.append(sum(x))\n",
        "    else:\n",
        "        with open('/content/drive/My Drive/Downloads/start_follow_read/data/train_a/0000'+str(i+1)+'/0000'+str(i+1)+'.json') as gt:\n",
        "            gt_json = json.load(gt)\n",
        "            x = [True for i in gt_json if 'sol' in i]\n",
        "            tmp.append(sum(x))\n",
        "max(tmp)            "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g87Ab40FrMQp",
        "colab_type": "text"
      },
      "source": [
        "#### batch_gen()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6DeulTIQ6my",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_gen():\n",
        "    images = []\n",
        "    labels = []\n",
        "    batch_size = 1\n",
        "    for instance in range(batch_size):\n",
        "        with open(\"/content/drive/My Drive/Downloads/start_follow_read/data/train_a_training_set.json\") as json_file:\n",
        "            train_list = json.load(json_file)\n",
        "        i = np.random.randint(0,len(train_list))\n",
        "        #print(i)\n",
        "        org_img = cv2.imread(\"/content/drive/My Drive/Downloads/start_follow_read/\"+ train_list[i][1])\n",
        "        gt_json = json_state(\"/content/drive/My Drive/Downloads/start_follow_read/\" + train_list[i][0])\n",
        "        target_dim1 = int(np.random.uniform(384,640))\n",
        "        s = target_dim1 / float(org_img.shape[1])\n",
        "        target_dim0 = int(org_img.shape[0]/float(org_img.shape[1]) * target_dim1)\n",
        "        re_img = cv2.resize(org_img,(target_dim1, target_dim0), interpolation = cv2.INTER_CUBIC)\n",
        "        gt = np.zeros((1, len(gt_json),4), dtype=np.float32)\n",
        "        for j , gt_item in enumerate(gt_json):\n",
        "            #print(gt_item)\n",
        "            if 'sol' not in gt_item:\n",
        "                continue\n",
        "            x0 = gt_item['sol']['x0']\n",
        "            x1 = gt_item['sol']['x1']\n",
        "            y0 = gt_item['sol']['y0']\n",
        "            y1 = gt_item['sol']['y1']\n",
        "\n",
        "            gt[:,j,0] = x0 * s + 256\n",
        "            gt[:,j,1] = y0 * s + 256\n",
        "            gt[:,j,2] = x1 * s + 256\n",
        "            gt[:,j,3] = y1 * s + 256\n",
        "        padded_img = np.pad(re_img, ((256,256),(256,256),(0,0)), 'mean')\n",
        "        pretrain_config = config['pretraining']\n",
        "        crop_params, patch_img, gt_match = generate_random_crop(padded_img, gt, pretrain_config['sol']['crop_params'])\n",
        "        patch_img = patch_img.astype(np.float32)\n",
        "        patch_img = patch_img / 128.0 - 1.0\n",
        "        gt = gt[gt_match][None,...]\n",
        "        gt[...,0] = gt[...,0] - crop_params['dim1'][0]\n",
        "        gt[...,1] = gt[...,1] - crop_params['dim0'][0]\n",
        "        gt[...,2] = gt[...,2] - crop_params['dim1'][0]\n",
        "        gt[...,3] = gt[...,3] - crop_params['dim0'][0]\n",
        "        images.append(patch_img.astype(np.float32))\n",
        "        gt_padded = np.zeros((1,64,4))\n",
        "        if gt.shape[1] == 0:\n",
        "            gt_padded = gt_padded\n",
        "        else:\n",
        "            gt_padded[:, 0:gt.shape[1], :] = gt \n",
        "        labels.append(gt_padded)\n",
        "    yield np.array(images) , np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA18v__rrTly",
        "colab_type": "text"
      },
      "source": [
        "#### dataset object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrgDYvKSVj2c",
        "colab_type": "code",
        "outputId": "40fc6f24-ff04-49f9-d237-e6c8104c8912",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset = tf.data.Dataset.from_generator(batch_gen, output_types=(tf.float32,tf.float32))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQz190rRrYwz",
        "colab_type": "text"
      },
      "source": [
        "#### val_data_gen()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSayi_9o8skw",
        "colab_type": "code",
        "outputId": "78b35881-8943-43fe-ef84-ec8453e90a57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def val_data_gen():\n",
        "    images = []\n",
        "    labels = []\n",
        "    batch_size = 5\n",
        "    #for instance in range(batch_size):\n",
        "    with open(\"/content/drive/My Drive/Downloads/start_follow_read/data/train_a_validation_set.json\") as json_file:\n",
        "        val_list = json.load(json_file)\n",
        "        for element in val_list:\n",
        "            org_img = cv2.imread(\"/content/drive/My Drive/Downloads/start_follow_read/\"+ element[1])\n",
        "            gt_json = json_state(\"/content/drive/My Drive/Downloads/start_follow_read/\" + element[0])\n",
        "            target_dim1 = int(np.random.uniform(512,512))\n",
        "            s = target_dim1 / float(org_img.shape[1])\n",
        "            target_dim0 = int(org_img.shape[0]/float(org_img.shape[1]) * target_dim1)\n",
        "            re_img = cv2.resize(org_img,(target_dim1, target_dim0), interpolation = cv2.INTER_CUBIC)\n",
        "            re_img = re_img.astype(np.float32)\n",
        "            re_img = re_img / 128.0 - 1.0 \n",
        "            gt = np.zeros((1, len(gt_json),4), dtype=np.float32)\n",
        "            for j , gt_item in enumerate(gt_json):\n",
        "                if 'sol' not in gt_item:\n",
        "                    continue\n",
        "                x0 = gt_item['sol']['x0']\n",
        "                x1 = gt_item['sol']['x1']\n",
        "                y0 = gt_item['sol']['y0']\n",
        "                y1 = gt_item['sol']['y1']\n",
        "\n",
        "                gt[:,j,0] = x0 * s\n",
        "                gt[:,j,1] = y0 * s\n",
        "                gt[:,j,2] = x1 * s\n",
        "                gt[:,j,3] = y1 * s\n",
        "            images.append(re_img)\n",
        "            labels.append(gt)\n",
        "        return images , labels\n",
        "val = val_data_gen()\n",
        "for cnt , (img, gt) in enumerate(zip(val[0] , val[1])):\n",
        "    print(\"{} image shape : {} , gt shape : {}\".format(cnt, img.shape , gt.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 image shape : (809, 512, 3) , gt shape : (1, 45, 4)\n",
            "1 image shape : (649, 512, 3) , gt shape : (1, 29, 4)\n",
            "2 image shape : (804, 512, 3) , gt shape : (1, 29, 4)\n",
            "3 image shape : (805, 512, 3) , gt shape : (1, 21, 4)\n",
            "4 image shape : (659, 512, 3) , gt shape : (1, 19, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI_Gs-XisZZW",
        "colab_type": "text"
      },
      "source": [
        "#### model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNofgGjE4ryJ",
        "colab_type": "code",
        "outputId": "b65f7a42-e59b-4676-8708-75772b238df9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "inputs = tf.keras.Input(shape=(None,None,3))\n",
        "conv1 = tf.keras.layers.Conv2D(64,kernel_size=3,padding='same',activation='relu' , kernel_initializer = tf.keras.initializers.glorot_normal(), bias_initializer=tf.keras.initializers.Zeros())(inputs)\n",
        "pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides=2)(conv1)\n",
        "conv2 = tf.keras.layers.Conv2D(128,kernel_size=3,padding='same',activation='relu', kernel_initializer = tf.keras.initializers.glorot_normal(),bias_initializer=tf.keras.initializers.Zeros())(pool1)\n",
        "pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides=2)(conv2)\n",
        "conv3 = tf.keras.layers.Conv2D(256,kernel_size=3,padding='same',activation='relu' , kernel_initializer = tf.keras.initializers.glorot_normal(),bias_initializer=tf.keras.initializers.Zeros())(pool2)\n",
        "conv4 = tf.keras.layers.Conv2D(256,kernel_size=3,padding='same',activation='relu' , kernel_initializer = tf.keras.initializers.glorot_normal(),bias_initializer=tf.keras.initializers.Zeros())(conv3)\n",
        "pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides=2)(conv4)\n",
        "conv5 = tf.keras.layers.Conv2D(512,kernel_size=3,padding='same',activation='relu' , kernel_initializer = tf.keras.initializers.glorot_normal(),bias_initializer=tf.keras.initializers.Zeros())(pool3)\n",
        "conv6 = tf.keras.layers.Conv2D(512,kernel_size=3,padding='same',activation='relu' , kernel_initializer = tf.keras.initializers.glorot_normal(),bias_initializer=tf.keras.initializers.Zeros())(conv5)\n",
        "pool4 = tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides=2)(conv6)\n",
        "conv7 = tf.keras.layers.Conv2D(512,kernel_size=3,padding='same',activation='relu' , kernel_initializer = tf.keras.initializers.glorot_normal(),bias_initializer=tf.keras.initializers.Zeros())(pool4)\n",
        "outputs = tf.keras.layers.Conv2D(5,kernel_size=3,padding='same')(conv7)\n",
        "perm = tf.keras.layers.Permute((3,1,2))(outputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op TruncatedNormal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DDQcqw0G7wn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictions(x):\n",
        "    priors_0 = (tf.range(tf.to_float(tf.shape(x)[2])) + 0.5) * 16\n",
        "    priors_0 = tf.reshape(priors_0, tf.convert_to_tensor([1, tf.shape(priors_0)[0], 1]))\n",
        "    priors_0 = tf.broadcast_to(priors_0, shape= tf.convert_to_tensor([1, tf.shape(priors_0)[1], tf.shape(x)[3] ]))\n",
        "    priors_0 = tf.expand_dims(priors_0, axis=0)\n",
        "    priors_1 = (tf.range(tf.to_float(tf.shape(x)[3])) + 0.5) * 16\n",
        "    priors_1 = tf.reshape(priors_1, tf.convert_to_tensor([1, 1, tf.shape(priors_1)[0]]))\n",
        "    priors_1 = tf.broadcast_to(priors_1, shape= tf.convert_to_tensor([1, tf.shape(x)[2], tf.shape(priors_1)[2]] ))\n",
        "    priors_1 = tf.expand_dims(priors_1, axis=0)\n",
        "    predictions = tf.concat( [tf.math.sigmoid(x[:,0:1,:,:]) ,\n",
        "                             x[:,1:2,:,:] + priors_0 ,\n",
        "                             x[:,2:3,:,:] + priors_1 ,\n",
        "                             x[:,3:4,:,:] ,\n",
        "                             x[:,4:5,:,:] ], axis = 1)\n",
        "    predictions = tf.transpose(predictions , perm = [0,3,2,1])\n",
        "    predictions = tf.reshape(predictions, shape = [tf.shape(predictions)[0],-1,5])\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhWnQE50LSiA",
        "colab_type": "code",
        "outputId": "0d5b354d-6480-40d6-c08f-fc3dccbf09cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "second_last_layer_lambda = tf.keras.layers.Lambda(get_predictions)\n",
        "second_last_layer = second_last_layer_lambda(perm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-14-db2f2112ae64>:2: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iterQvKVK0o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def xyrs_2_xyxy(p):\n",
        "    x = p[:,:,1:2]\n",
        "    y = p[:,:,2:3]\n",
        "    r = p[:,:,3:4]\n",
        "    s = p[:,:,4:5]\n",
        "    x0 = -tf.math.sin(r) * s + x\n",
        "    y0 = -tf.math.cos(r) * s + y\n",
        "    x1 =  tf.math.sin(r) * s + x\n",
        "    y1 =  tf.math.cos(r) * s + y\n",
        "    predictions_xyxy = tf.concat([ p[:,:,0:1],\n",
        "                              x0, y0, x1, y1 ], axis=2)\n",
        "    return predictions_xyxy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHlMJKQtHod2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_layer_lambda = tf.keras.layers.Lambda(xyrs_2_xyxy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pug6WJ_IpEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_layer = last_layer_lambda(second_last_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NViDFqJLC34W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Model(inputs=inputs, outputs=last_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3bz8nqvI22P",
        "colab_type": "code",
        "outputId": "fe865227-9fb6-49a9-9d31-af0b005bb1c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None, None, 3)]   0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, None, None, 5)     23045     \n",
            "_________________________________________________________________\n",
            "permute (Permute)            (None, 5, None, None)     0         \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, None, 5)           0         \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, None, 5)           0         \n",
            "=================================================================\n",
            "Total params: 6,883,717\n",
            "Trainable params: 6,883,717\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDRW1EuUN5m7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khfNjWHl_zci",
        "colab_type": "text"
      },
      "source": [
        "#### alignment_loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctUXclkOQ7u-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def alignment_loss(y_true, y_pred):\n",
        "    nz_sol_idx = [i for i,j in enumerate(y_true) if np.nonzero(j.numpy())[0].size is not 0]\n",
        "    z_sol_idx = [i for i,j in enumerate(y_true) if np.nonzero(j.numpy())[0].size is 0]\n",
        "    if len(z_sol_idx) != 0:\n",
        "        z_sol_predictions = tf.gather(params = y_pred , indices = z_sol_idx)\n",
        "        z_sol_confidences = z_sol_predictions[:,:,0]\n",
        "        z_sol_log_one_minus_confidences = tf.math.log(1.0 -  z_sol_confidences + 1e-10)\n",
        "        z_sol_loss = tf.math.divide(x = -tf.math.reduce_sum(z_sol_log_one_minus_confidences) , y = len(z_sol_idx))\n",
        "    if len(nz_sol_idx) != 0:\n",
        "        nz_sol_predictions = tf.gather(params = y_pred, indices = nz_sol_idx) \n",
        "        nz_sol_confidences = nz_sol_predictions[:,:,0]\n",
        "        nz_sol_log_one_minus_confidences = tf.math.log(1.0 -  nz_sol_confidences + 1e-10)\n",
        "    else:\n",
        "        return z_sol_loss\n",
        "    nz_sol_predictions = tf.expand_dims( nz_sol_predictions, axis=1)\n",
        "    nz_sol_confidences = tf.expand_dims(nz_sol_confidences , axis=1)\n",
        "    nz_sol_log_one_minus_confidences = tf.expand_dims(nz_sol_log_one_minus_confidences , axis=1)\n",
        "    nz_sol_y_true = tf.gather(params = y_true , indices = nz_sol_idx)\n",
        "    nz_sol_loss = []\n",
        "    for i in range(len(nz_sol_idx)):\n",
        "        locations = nz_sol_predictions[i][:,:,1:5]\n",
        "        y_true = nz_sol_y_true[i]\n",
        "        int_ten = tf.math.reduce_sum(y_true , axis =2)\n",
        "        z_v = tf.zeros(shape=(1, y_true.shape[1]) , dtype = tf.float32)\n",
        "        b_m = tf.not_equal(int_ten , z_v)\n",
        "        o_z = tf.boolean_mask(y_true , b_m)\n",
        "        y_true = tf.expand_dims(o_z , axis = 0)\n",
        "        label_sizes = tf.shape(input = y_true)[1]\n",
        "        log_confidences = tf.math.log(nz_sol_confidences[i] + 1e-10)\n",
        "        expanded_locations = tf.expand_dims(locations, axis=2)\n",
        "        expanded_target = tf.expand_dims(y_true, axis=1)\n",
        "        expanded_locations = tf.broadcast_to( expanded_locations, shape= [tf.shape(input = locations)[0],\n",
        "                                                                        tf.shape(input = locations)[1],\n",
        "                                                                        tf.shape(input = y_true)[1],\n",
        "                                                                        tf.shape(input = locations)[2]])\n",
        "        expanded_target = tf.broadcast_to( expanded_target, shape= [tf.shape(input = y_true)[0],\n",
        "                                                                    tf.shape(input = locations)[1],\n",
        "                                                                    tf.shape(input = y_true)[1],\n",
        "                                                                    tf.shape(input = y_true)[2]])\n",
        "        location_deltas = (expanded_locations - expanded_target)\n",
        "        normed_difference = tf.norm(location_deltas,ord='euclidean',axis=3)**2\n",
        "        expanded_log_confidences = tf.expand_dims(log_confidences , axis=2)\n",
        "        expanded_log_confidences = tf.broadcast_to(expanded_log_confidences , shape=[tf.shape(input = locations)[0],\n",
        "                                                                                    tf.shape(input = locations)[1],\n",
        "                                                                                    tf.shape(input = y_true)[1]])\n",
        "        expanded_log_one_minus_confidences = tf.expand_dims(nz_sol_log_one_minus_confidences[i] , axis=2)\n",
        "        expanded_log_one_minus_confidences = tf.broadcast_to(expanded_log_one_minus_confidences , shape=[tf.shape(input = locations)[0],\n",
        "                                                                                                        tf.shape(input = locations)[1],\n",
        "                                                                                                        tf.shape(input = y_true)[1]])\n",
        "        alpha_alignment= 0.1\n",
        "        alpha_backprop= 0.1\n",
        "        C = alpha_alignment/2.0 * normed_difference - expanded_log_confidences + expanded_log_one_minus_confidences\n",
        "        X = tf.zeros_like(C)\n",
        "        X = X.numpy()\n",
        "        C_i = C[0,:,:label_sizes]\n",
        "        row_ind, col_ind = linear_sum_assignment(tf.transpose(C_i))\n",
        "        X[0][(col_ind, row_ind)] = 1.0\n",
        "        X = tf.Variable(initial_value = X , dtype = tf.float32)\n",
        "        X2 = 1.0 - tf.math.reduce_sum(X, 2)\n",
        "        location_loss = tf.math.reduce_sum((alpha_backprop/2.0 * normed_difference * X))\n",
        "        confidence_loss =  -tf.math.reduce_sum((expanded_log_confidences * X)) - tf.math.reduce_sum((nz_sol_log_one_minus_confidences[i] * X2))\n",
        "        loss = confidence_loss + location_loss\n",
        "        nz_sol_loss.append(loss)\n",
        "    nz_sol_loss = tf.math.divide(x = tf.math.reduce_sum(nz_sol_loss) , y = len(nz_sol_loss))\n",
        "    if len(z_sol_idx) == 0 :\n",
        "        return nz_sol_loss\n",
        "    else :\n",
        "        return tf.divide(x = z_sol_loss + nz_sol_loss , y = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61-IoZqJb8NX",
        "colab_type": "text"
      },
      "source": [
        "#### val_loss()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwVwluv8oasn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def val_loss():\n",
        "    validation_loss = []\n",
        "    for i in range(5):\n",
        "        #print(\"image : {} , image shape : {}\".format(i , val[0][i].shape))\n",
        "        #print(\"gt : {} , gt shape : {}\".format(i , val[1][i].shape))\n",
        "        y_hat = model(tf.expand_dims(tf.transpose(val[0][i] , (1,0,2)) , axis = 0))\n",
        "        y = tf.expand_dims(val[1][i] , axis = 0) \n",
        "        validation_loss.append(alignment_loss(y , y_hat))\n",
        "    return tf.divide( x = tf.math.reduce_sum(np.array(validation_loss)) , y = 5 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6PpBXWYcDRW",
        "colab_type": "text"
      },
      "source": [
        "#### training preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu41b5iAU1x0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, epsilon = 1e-08)\n",
        "train_loss_history = []\n",
        "#validation_loss_history = []\n",
        "def train_step(image, y_true):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(image, training=True)\n",
        "        loss_value = alignment_loss( y_true, y_pred)\n",
        "        #print(\"Train Loss : {}\".format(loss_value))\n",
        "        #print(\"Validation Loss : {}\".format(val_loss()))\n",
        "    #validation_loss_history.append(val_loss().numpy())\n",
        "    train_loss_history.append(loss_value.numpy().mean())\n",
        "    #training_loss.append(loss_value.numpy().mean())\n",
        "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0ovrUNwU1jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import os\n",
        "def train(epochs):\n",
        "    if os.path.exists('/content/drive/My Drive/sol_tf.h5'):\n",
        "        print(\"Loading best saved model ...\")\n",
        "        model.load_weights('/content/drive/My Drive/sol_tf.h5')\n",
        "        print('Validation Loss : {}'.format(val_loss()))\n",
        "        print(\"Loaded :-)\")\n",
        "        lowest_loss = val_loss()\n",
        "    else:\n",
        "        lowest_loss = np.inf\n",
        "    cnt_since_last_improvement = 0  \n",
        "    val_loss_history = []\n",
        "    for epoch in range(epochs):\n",
        "        #training_loss = []\n",
        "        st = time.time()\n",
        "        for (batch, (image, y_true)) in enumerate(dataset.repeat(1000)):\n",
        "            print(\"Epoch : {} , Batch : {}\".format(epoch , batch))\n",
        "            sta = time.time()\n",
        "            train_step(image, y_true)\n",
        "            print(\"Time taken by Epoch {} Batch {} : {} seconds\".format(epoch , batch , time.time() - sta ))\n",
        "            print(\"ETA at Epoch End : {} seconds \".format( (time.time() - sta) * (1000 - batch + 1) ))\n",
        "        print ('Epoch {} finished'.format(epoch))\n",
        "        print(\"Time taken by Epoch {} : {} seconds\".format(epoch , time.time() - st))\n",
        "        #print(\"Training Loss : {}\".format(sum(training_loss)/1000))\n",
        "        print(\"Validation Loss : {}\".format(val_loss()))\n",
        "        val_loss_history.append(val_loss())\n",
        "        cnt_since_last_improvement += 1\n",
        "        if lowest_loss > val_loss():\n",
        "            cnt_since_last_improvement = 0\n",
        "            lowest_loss = val_loss()\n",
        "            print(\"Saving Best\")\n",
        "            model.save('/content/drive/My Drive/sol_tf.h5')\n",
        "        if cnt_since_last_improvement >= 10:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBjFlVz4fxSB",
        "colab_type": "text"
      },
      "source": [
        "#### traning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZCF2RF9M3qO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = time.time()\n",
        "train(epochs = 1000)\n",
        "print(\"Total time taken : {}\".format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRyoHFGjf2JM",
        "colab_type": "text"
      },
      "source": [
        "#### tmp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3YuV5wnJQrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "sns.lineplot(x = list(range(len(train_loss_history))) , y = train_loss_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtI1G6hBVMVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.lineplot(x = list(range(len(validation_loss_history))) , y = validation_loss_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-nHRb-KVZvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.sort(after_predict(model.predict(tf.expand_dims(tf.transpose(val[0][0] , (1,0,2)) , axis = 0)))[...,0:1].numpy(), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtphX6q7WA7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def after_predict(y_pred):\n",
        "    batch_size = tf.shape(input = y_pred)[0]\n",
        "    base_0 , base_1 = tf.constant(16, dtype=tf.float32), tf.constant(16, tf.float32)\n",
        "    priors_0 = (tf.constant(np.arange(tf.shape(y_pred)[2]) , dtype=tf.float32) + tf.constant(0.5, dtype=tf.float32)) * base_0\n",
        "    priors_0 = tf.reshape(priors_0, [1, priors_0.shape[0], 1])\n",
        "    priors_0 = tf.broadcast_to(priors_0, shape=[1, priors_0.shape[1], y_pred.shape[3]])\n",
        "    priors_0 = tf.expand_dims(priors_0, axis=0)\n",
        "    #print(\"priors_0 : {}\".format(priors_0))\n",
        "    priors_1 = (tf.constant(np.arange(tf.shape(y_pred)[3]), dtype = tf.float32) + tf.constant(0.5, dtype=tf.float32)) * base_1\n",
        "    priors_1 = tf.reshape(priors_1, [1, 1,priors_1.shape[0]])\n",
        "    priors_1 = tf.broadcast_to(priors_1, shape=[1, y_pred.shape[2], priors_1.shape[2]])\n",
        "    priors_1 = tf.expand_dims(priors_1, axis=0)\n",
        "    #print(\"priors_1 : {}\".format(priors_1))\n",
        "    predictions = tf.concat( [tf.math.sigmoid(y_pred[:,0:1,:,:]) ,\n",
        "                             y_pred[:,1:2,:,:] + priors_0 ,\n",
        "                             y_pred[:,2:3,:,:] + priors_1 ,\n",
        "                             y_pred[:,3:4,:,:] ,\n",
        "                             y_pred[:,4:5,:,:] ], axis = 1)\n",
        "    #print(\"predictions shape: {}\".format(predictions.shape))\n",
        "    predictions = tf.transpose(predictions , perm = [0,3,2,1])\n",
        "    #print(\"predictions shape: {}\".format(predictions.shape))\n",
        "    predictions = tf.reshape(predictions, shape = [batch_size,-1,5])\n",
        "    #print(\"predictions shape: {}\".format(predictions.shape))\n",
        "    x = predictions[:,:,1:2]\n",
        "    y = predictions[:,:,2:3]\n",
        "    r = predictions[:,:,3:4]\n",
        "    s = predictions[:,:,4:5]\n",
        "    x0 = -tf.math.sin(r) * s + x\n",
        "    y0 = -tf.math.cos(r) * s + y\n",
        "    x1 =  tf.math.sin(r) * s + x\n",
        "    y1 =  tf.math.cos(r) * s + y\n",
        "    predictions = tf.concat([ predictions[:,:,0:1],\n",
        "                              x0, y0, x1, y1 ], axis=2)\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}